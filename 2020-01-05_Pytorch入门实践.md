# PyTorch入门实践

## 1.为什么分类问题不采用最大化准确率为优化目标呢？

​	存在以下两个问题

1. 权重变化但是准确率不变化的时候，梯度为0
2. 准确率变化的时候梯度不连续

## 2.为什么逻辑回归 被称为回归

- 一般认为采用mse的被认为是回归
- 交叉熵的被认为是分类

## 3.影响训练效果的因素

- 学习率
- 梯度弥散
- 初始化

## 4.正则化

![image-20200105221645525](https://tva1.sinaimg.cn/large/006tNbRwly1gam1soivu2j30zu05umy0.jpg)

![image-20200105221717025](https://tva1.sinaimg.cn/large/006tNbRwly1gam1sm6b7tj30y20f0taw.jpg)

## 5.Momentum和学习率衰减

![image-20200105221810992](https://tva1.sinaimg.cn/large/006tNbRwly1gam1siywefj31020a640g.jpg)

![image-20200105221832836](https://tva1.sinaimg.cn/large/006tNbRwly1gam1sddsytj31020g077c.jpg)

![image-20200105221854644](https://tva1.sinaimg.cn/large/006tNbRwly1gam1s982s6j30xu0f8wfq.jpg)

## 6.dropout

![image-20200105221930154](https://tva1.sinaimg.cn/large/006tNbRwly1gam1rmawanj30z40egwnf.jpg)

## 7. visdom可视化

```shell
# 启动监视器
 python -m visdom.server
```

```python
from visdom import Visdom
viz = Visdom()
viz.line([0.], [0.], win='train_loss', opts=dict(title='train loss'))
viz.line([loss.item()], [global_step], win='train_loss', update='append')

viz.line([[0.0, 0.0]], [0.], win='test', opts=dict(title='test loss&acc', legend=['loss', 'acc']))

viz.line([[test_loss, 100. * correct.cpu().numpy() / len(test_loader.dataset)]],
          [global_step],
          win='test',
          update='append')
  
viz.images(data.view(-1, 1, 28, 28), win='x')
viz.text(str(pred.detach().cpu().numpy()), win='pred', opts=dict(title='pred'))
```

