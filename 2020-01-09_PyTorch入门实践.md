# PyTorch入门实践

## 一、ResNet

### 1.ResNet解释

![image-20200109162239649](https://tva1.sinaimg.cn/large/006tNbRwly1gaqdxnaxs9j311s0u01kx.jpg)

![image-20200109162302365](https://tva1.sinaimg.cn/large/006tNbRwly1gaqef76bosj313o0u078l.jpg)

### 2.ResNet的PyTorch实现

```python
import torch.nn as nn
import torch.nn.functional as F


class ResBlk(nn.module):
    def __init__(self, ch_in, ch_out):
        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(ch_out)
        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(ch_out)
        self.extra = nn.Sequential()
        if ch_out != ch_in:
            self.extra = nn.Sequential(
                nn.Conv2d(ch_in, ch_out, kernel_size=1, stride=1),
                nn.BatchNorm2d(ch_out)
            )

    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.extra(x) + out
        return out
```



## 二、DenseNet

![image-20200109163921381](https://tva1.sinaimg.cn/large/006tNbRwly1gaqef338x4j31hc0u0dix.jpg)

## 三、nn.Module模块

### 1. 内嵌现成的函数层

1. Linear
2. ReLU
3. Sigmoid
4. Conv2d
5. ConvTransposed2d
6. Dropout
7. Etc

### 2.Container

```python
net = nn.Sequential(
    nn.Conv2d(1, 32, 5, 1, 1),
    nn.MaxPool2d(2, 2),
    nn.ReLU(True),
    nn.BatchNorm2d(32),

    nn.Conv2d(32, 64, 3, 1, 1),
    nn.ReLU(True),
    nn.BatchNorm2d(64),

    nn.Conv2d(64, 64, 3, 1, 1),
    nn.MaxPool2d(2, 2),
    nn.ReLU(True),
    nn.BatchNorm2d(64),

    nn.Conv2d(64, 128, 3, 1, 1),
    nn.ReLU(True),
    nn.BatchNorm2d(128),
)
```

### 3.parameters

![image-20200109164859029](https://tva1.sinaimg.cn/large/006tNbRwly1gaqep4gzqmj31hc0u0n2n.jpg)

### 4.modules

![image-20200109164931712](https://tva1.sinaimg.cn/large/006tNbRwly1gaqepofr39j31hc0u0kjl.jpg)

![image-20200109165015120](https://tva1.sinaimg.cn/large/006tNbRwly1gaqeqlu5p0j31hc0u0npe.jpg)

![image-20200109165043265](https://tva1.sinaimg.cn/large/006tNbRwly1gaqer4oy3cj31hc0u0q6k.jpg)

### 5.to(device)

```python
device = torch.device('cuda')
net = Net()
net.to(device)

```

### 6.save and load

```python
net.load_state_dict(torch.load('ckpt.mdl'))

# train
torch.save(net.state_dict(), 'ckpt.mdl')
```

### 7.train and test

```python
# train
net.train()
# test
net.eval()
```

### 8.implement our layer

```python

class Flatten(nn.Module):
    def __init__(self):
        super(Flatten, self).__init__()

    def forward(self, input):
        return input.view(input.size(0), -1)


class TestNet(nn.Module):
    def __init__(self):
        super(TestNet, self).__init__()
        self.net = nn.Sequential(
            nn.Conv2d(1, 16, stride=1, padding=1),
            nn.MaxPool2d(2, 2),
            Flatten(),
            nn.Linear(1 * 14 * 14, 10)
        )

    def forward(self, x):
        return self.net(x)

```

```python

class MyLinear(nn.Module):
    def __init__(self, inp, outp):
        super(MyLinear, self).__init__()

        self.w = nn.Parameter(torch.randn(outp, inp))
        self.b = nn.Parameter(torch.randn(outp))

    def forward(self, x):
        x = x @ self.w.t() + self.b
        return x
```

## 四、数据增强

```python

train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.RandomHorizontalFlip(),
                       transforms.RandomVerticalFlip(),
                       transforms.RandomRotation(15),  # -15<0<15
                       transforms.RandomRotation(90, 180, 270),  # 三个角度随机选一个旋转
                       transforms.Resize([32, 32]),
                       transforms.RandomCrop([28, 28]),
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ])),
    batch_size=BATCH_SIZE, shuffle=True)
```

